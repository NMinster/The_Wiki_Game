{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7ecd17-acad-4e69-a72c-9d23e9734d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "\n",
    "def find_shortest_path(start, end):\n",
    "    path_from_start = {start: [start]}\n",
    "    path_from_end = {end: [end]}\n",
    "    Q_from_start = deque([start])\n",
    "    Q_from_end = deque([end])\n",
    "\n",
    "    while Q_from_start and Q_from_end:\n",
    "        page_from_start = Q_from_start.popleft()\n",
    "        page_from_end = Q_from_end.popleft()\n",
    "\n",
    "        links_from_start = get_links(page_from_start)\n",
    "        links_from_end = get_links(page_from_end)\n",
    "\n",
    "        # check for intersection from start to end\n",
    "        for link in links_from_start:\n",
    "            if link in path_from_end:\n",
    "                return path_from_start[page_from_start] + path_from_end[link][::-1]\n",
    "\n",
    "            if (link not in path_from_start) and (link != page_from_start):\n",
    "                path_from_start[link] = path_from_start[page_from_start] + [link]\n",
    "                Q_from_start.append(link)\n",
    "\n",
    "        # check for intersection from end to start\n",
    "        for link in links_from_end:\n",
    "            if link in path_from_start:\n",
    "                return path_from_start[link] + path_from_end[page_from_end][::-1]\n",
    "\n",
    "            if (link not in path_from_end) and (link != page_from_end):\n",
    "                path_from_end[link] = path_from_end[page_from_end] + [link]\n",
    "                Q_from_end.append(link)\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_links(page):\n",
    "    r = requests.get(page)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    base_url = page[:page.find('/wiki/')]\n",
    "    links = list({base_url + a['href'] for a in soup.select('p a[href]') if a['href'].startswith('/wiki/')})\n",
    "    return links\n",
    "\n",
    "def check_pages(start, end):\n",
    "    languages = []\n",
    "    for page in [start, end]:\n",
    "        try:\n",
    "            ind = page.find('.wikipedia.org/wiki/')\n",
    "            languages.append(page[(ind-2):ind])\n",
    "            requests.get(page)\n",
    "        except:\n",
    "            print(f'{page} does not appear to be a valid Wikipedia page.')\n",
    "            return False\n",
    "\n",
    "    if len(set(languages)) > 1:\n",
    "        print('Pages are in different languages.')\n",
    "        return False\n",
    "\n",
    "    if len(get_links(start)) == 0:\n",
    "        print('Start page is a dead-end page with no Wikipedia links.')\n",
    "        return False\n",
    "\n",
    "    end_soup = BeautifulSoup(requests.get(end).content, 'html.parser')\n",
    "    if end_soup.find('table', {'class': 'metadata plainlinks ambox ambox-style ambox-Orphan'}):\n",
    "        print('End page is an orphan page with no Wikipedia pages linking to it.')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def redirected(end):\n",
    "    end_soup = BeautifulSoup(requests.get(end).content, 'html.parser')\n",
    "    title = end_soup.find('h1').text\n",
    "    title = title.replace(' ', '_', len(title))\n",
    "    base_url = end[:end.find('/wiki/') + len('/wiki/')]\n",
    "    return set([end, base_url + title])\n",
    "\n",
    "def result(start, end, path):\n",
    "    if path:\n",
    "        result = path\n",
    "    else:\n",
    "        result = \"No path! :( \"\n",
    "    d = {\"start\": start, \"end\": end, \"path\": result}\n",
    "    return json.dumps(d, indent=4)\n",
    "\n",
    "def main(input_json):\n",
    "    data = json.loads(input_json)\n",
    "    start = data[\"start\"]\n",
    "    end = data[\"end\"]\n",
    "    if check_pages(start, end):\n",
    "        path = find_shortest_path(start, end)\n",
    "        json_result = result(start, end, path)\n",
    "        return json_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598891d3-bfb3-4be5-b8f2-36affcd1ad91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"start\": \"https://en.wikipedia.org/wiki/2023_South_Korea_floods\",\n",
      "    \"end\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
      "    \"path\": [\n",
      "        \"https://en.wikipedia.org/wiki/2023_South_Korea_floods\",\n",
      "        \"https://en.wikipedia.org/wiki/Joseon\",\n",
      "        \"https://en.wikipedia.org/wiki/Wikipedia:Citation_needed\",\n",
      "        \"https://en.wikipedia.org/wiki/Rodney_Brooks\",\n",
      "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
      "    ]\n",
      "}\n",
      "Time: 0.03333333333333333m 2.354s\n"
     ]
    }
   ],
   "source": [
    "input_json = '{\"start\": \"https://en.wikipedia.org/wiki/2023_South_Korea_floods\", \"end\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\"}'\n",
    "starttime = time.time()\n",
    "\n",
    "print(main(input_json))\n",
    "\n",
    "endtime = time.time()\n",
    "totaltime = endtime - starttime\n",
    "print('Time: {}m {:.3f}s'.format(int(totaltime)/60, totaltime%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f01e03a-702c-46af-8a06-27cb9abb5b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "\n",
    "def find_shortest_path(start, end):\n",
    "    path = {}\n",
    "    path[start] = [start]\n",
    "    Q = deque([start])\n",
    "    while len(Q) != 0:\n",
    "        page = Q.popleft()\n",
    "        links = get_links(page)\n",
    "        for link in links:\n",
    "            if link in end:\n",
    "                return path[page] + [link]\n",
    "            if (link not in path) and (link != page):\n",
    "                path[link] = path[page] + [link]\n",
    "                Q.append(link)\n",
    "    return None\n",
    "\n",
    "def get_links(page):\n",
    "    r = requests.get(page)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    base_url = page[:page.find('/wiki/')]\n",
    "    links = list({base_url + a['href'] for a in soup.select('p a[href]') if a['href'].startswith('/wiki/')})\n",
    "    return links\n",
    "\n",
    "def check_pages(start, end):\n",
    "    languages = []\n",
    "    for page in [start, end]:\n",
    "        try:\n",
    "            ind = page.find('.wikipedia.org/wiki/')\n",
    "            languages.append(page[(ind-2):ind])\n",
    "            requests.get(page)\n",
    "        except:\n",
    "            print(f'{page} does not appear to be a valid Wikipedia page.')\n",
    "            return False\n",
    "\n",
    "    if len(set(languages)) > 1:\n",
    "        print('Pages are in different languages.')\n",
    "        return False\n",
    "\n",
    "    if len(get_links(start)) == 0:\n",
    "        print('Start page is a dead-end page with no Wikipedia links.')\n",
    "        return False\n",
    "\n",
    "    end_soup = BeautifulSoup(requests.get(end).content, 'html.parser')\n",
    "    if end_soup.find('table', {'class': 'metadata plainlinks ambox ambox-style ambox-Orphan'}):\n",
    "        print('End page is an orphan page with no Wikipedia pages linking to it.')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def redirected(end):\n",
    "    end_soup = BeautifulSoup(requests.get(end).content, 'html.parser')\n",
    "    title = end_soup.find('h1').text\n",
    "    title = title.replace(' ', '_', len(title))\n",
    "    base_url = end[:end.find('/wiki/') + len('/wiki/')]\n",
    "    return set([end, base_url + title])\n",
    "\n",
    "def result(start, end, path):\n",
    "    if path:\n",
    "        result = path\n",
    "    else:\n",
    "        result = \"No path! :( \"\n",
    "    d = {\"start\": start, \"end\": end, \"path\": result}\n",
    "    return json.dumps(d, indent=4)\n",
    "\n",
    "def main(input_json):\n",
    "    data = json.loads(input_json)\n",
    "    start = data[\"start\"]\n",
    "    end = data[\"end\"]\n",
    "    if check_pages(start, end):\n",
    "        path = find_shortest_path(start, redirected(end))\n",
    "        json_result = result(start, end, path)\n",
    "        return json_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a50cd0-b690-4d6a-a7bf-4195d4edad3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_json = '{\"start\": \"https://en.wikipedia.org/wiki/2023_South_Korea_floods\", \"end\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\"}'\n",
    "starttime = time.time()\n",
    "\n",
    "print(main(input_json))\n",
    "\n",
    "endtime = time.time()\n",
    "totaltime = endtime - starttime\n",
    "print('Time: {}m {:.3f}s'.format(int(totaltime)/60, totaltime%60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a029283-13ce-4da8-bb0c-d145d5be92a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
